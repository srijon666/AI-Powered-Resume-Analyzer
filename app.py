import streamlit as st
import matplotlib.pyplot as plt
import numpy as np # Used for plotting
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2
import re
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import requests
import json
import time 
import os 

# --- NLTK Data Path Configuration (RELIABLE FIX) ---
# 1. Define the reliable path within the app's root directory.
NLTK_DATA_PATH = os.path.join(os.getcwd(), '.nltk_data')

# 2. Set the NLTK_DATA environment variable. This is the most reliable way 
#    to tell NLTK where to look for data and where to save it.
try:
    if not os.path.exists(NLTK_DATA_PATH):
        os.makedirs(NLTK_DATA_PATH, exist_ok=True)
    os.environ['NLTK_DATA'] = NLTK_DATA_PATH
    if NLTK_DATA_PATH not in nltk.data.path:
        nltk.data.path.append(NLTK_DATA_PATH)
except Exception as e:
    st.warning(f"Could not configure NLTK data path: {e}")
# ----------------------------------------------------

# Configuration and Constants
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    GEMINI_API_URL = st.secrets["GEMINI_API_URL"]
except KeyError as e:
    st.error(f"Configuration Error: Missing key '{e.args[0]}' in .streamlit/secrets.toml. Please ensure both GEMINI_API_KEY and GEMINI_API_URL are set.")
    GEMINI_API_KEY = None
    GEMINI_API_URL = None


# Import NLTK Resources
try:
    # Now NLTK knows where to save/find the data automatically via the NLTK_DATA environment variable.
    nltk.download("punkt", quiet=True)
    nltk.download("stopwords", quiet=True)
    nltk.download("averaged_perceptron_tagger", quiet=True)
except Exception as e:
    st.error(f"Error downloading NLTK resources: {e}")

# Page configuration
st.set_page_config(page_title = "AI-Powered Learning Path Analyzer",page_icon="ðŸ¤–",layout="wide")
st.title("AI-Powered Learning Path Analyzer")

st.markdown("""
Upload your resume (ðŸ“‘PDF) and job description to analyze compatibility, extract key skills, and generate a **personalized learning path** using the Gemini AI.
""")

with st.sidebar:
    st.header("About")
    st.info("""
This powerful tool helps you:
- **Analyze Compatibility:** Use traditional and AI techniques to score your resume against a job description. 
- **Identify Skill Gaps:** Pinpoint the exact skills you're missing.
- **Generate Learning Path:** Receive a **5-step, actionable plan** generated by Gemini to close your skill gaps.
- **Get Ranked:** Receive an AI-powered **Job Readiness Score (1-100)**.
""")
    st.header("How it works")
    st.write("""
1. Upload your resume (ðŸ“‘PDF)
2. Paste the job description text.
3. Click **Analyze Resume** to see the results. 
4. Review the compatibility score and your personalized AI learning path! 
""")

# Core Utility Functions

def extract_text_from_pdf(uploaded_file):
    try:
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text = text+page_text
        return text
    except Exception as e:
        st.error(f"Error reading PDF file: {e}")
        return ""

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]','',text)
    text = re.sub(r'\s+',' ',text).strip()
    return text

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    return " ".join([word for word in words if word not in stop_words])

def calculate_similaity(resume_text, job_description):
    resume_processed=remove_stopwords(clean_text(resume_text))
    job_processed=remove_stopwords(clean_text(job_description))     
    vectorizer = TfidfVectorizer()
    if not resume_processed or not job_processed:
        return 0, resume_processed, job_processed

    tfidf_matrix = vectorizer.fit_transform([resume_processed, job_processed])
    score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]*100
    return round(score,2),resume_processed,job_processed

def extract_keywords(text, num_keywords=10):
    words = word_tokenize(text)
    words = [w for w in words if len(w) > 2]
    tagged_words = pos_tag(words)
    nouns = [w for w, pos in tagged_words if pos.startswith('NN') or pos.startswith('JJ')]
    word_freq = Counter(nouns)
    return word_freq.most_common(num_keywords)



# Function to extract candidate details (Name, Email, Phone)
def extract_candidate_info(text):
    info = {
        "Name": "N/A",
        "Email": "N/A",
        "Phone": "N/A"
    }
    
    # Name extraction: Take the first non-empty line of the resume.

    first_lines = [line.strip() for line in text.split('\n') if line.strip()]
    if first_lines and len(first_lines[0].split()) >= 2: # Must be at least two words for a name
        info["Name"] = first_lines[0].strip()
    elif first_lines:
        # Fallback to single word if the first line is very short
        info["Name"] = first_lines[0].strip()
    
    # Email extraction (kept intact)
    email_match = re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
    if email_match:
        info["Email"] = email_match.group(0)
    
    # Phone number extraction: Enforce country code (optional) + 10 digits minimum.
    phone_regex = re.compile(
        r"""(\+?\d{1,4})? # Optional Country Code, e.g., +1 or 91
        [\s\-\.]? # Separator after CC
        (\(?\d{2,5}\)?[\s\-\.]?\d{2,5}[\s\-\.]?\d{4}) # 10+ digit number structure
        """, re.VERBOSE
    )
    
    phone_match = phone_regex.search(text)
    if phone_match:
        full_match_text = phone_match.group(0).strip()
        
        clean_check = re.sub(r'[\s\-\.\(\)]+', '', full_match_text)
        
        if len(clean_check) >= 10:
            info["Phone"] = full_match_text
    
    return info

def extract_section_content(text, start_header, end_headers):
    text_lines = text.upper().split('\n')
    text_normalized = '\n'.join(text_lines)
    
    end_pattern = '|'.join([re.escape(h) for h in end_headers])
    
    pattern = re.compile(rf'\b{re.escape(start_header)}\b(.*?)(?:\b{end_pattern}\b|\Z)', re.DOTALL | re.IGNORECASE)
    
    match = pattern.search(text_normalized)
    if match:
        return match.group(1).strip()
    return ""

def rank_skills_by_projects(skill_text, project_text):
    skills_raw = re.split(r'[\n,;â€¢\-() ]+', skill_text.lower())
    skills = set([s for s in skills_raw if len(s) > 2 and s not in stopwords.words('english') and not s.isdigit()])

    project_words = word_tokenize(clean_text(project_text))
    
    skill_counts = Counter()
    
    for skill in skills:
        count = project_words.count(skill)
        if count > 0:
            skill_counts[skill] = count
            
    return skill_counts.most_common()

def plot_density_curve(score):
    
    clamped_score = max(5, min(95, score)) 
    
    data = np.random.normal(clamped_score, 15, 1000)
    
    data = np.clip(data, 0, 100)
    
    fig, ax = plt.subplots(figsize=(6, 4))
    
    n, bins, patches = ax.hist(data, bins=30, density=True, alpha=0.0, color='white')

    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    
    window_size = 5
    density_smoothed = np.convolve(n, np.ones(window_size)/window_size, mode='same')
    
    if score < 40:
        fill_color = '#ff4b4b' # Red
    elif score < 70:
        fill_color = '#ffa726' # Orange
    else:
        fill_color = '#0f9d58' # Green
        
    ax.plot(bin_centers, density_smoothed, color=fill_color, linewidth=2)
    ax.fill_between(bin_centers, 0, density_smoothed, color=fill_color, alpha=0.3)
    
    ax.axvline(score, color='black', linestyle='--', linewidth=1.5, label=f'Overall Match: {score}%')
    
    ax.set_title("Skill Relevance Distribution (Simulated Density)")
    ax.set_xlabel("Relevance Score (%)")
    ax.set_ylabel("Density")
    ax.set_xlim(0, 100)
    ax.legend(loc='upper left')
    ax.grid(axis='y', linestyle='--', alpha=0.6)
    
    return fig

# Gemini API Call for Learning Path Generation (Intact)
def generate_learning_path(resume_text, job_description):
    """Calls the Gemini API to get a structured learning path recommendation."""
    
    
    if not GEMINI_API_KEY or not GEMINI_API_URL:
        return None

    # Define the AI's role and the required JSON structure
    system_prompt = (
        "You are an AI-Powered Career Coach and Learning Path Recommender. "
        "Analyze the provided Resume Text and Job Description to identify skill gaps. "
        "Your response MUST be a JSON object containing a JobReadinessScore (1-100), "
        "a concise Summary of strengths/gaps, and a 5-step, highly actionable LearningPathSteps array."
    )

    # User Query: The specific task
    user_query = (
        f"Analyze the candidate's resume against the job description. "
        f"RESUME (Snippet): '{resume_text[:1500]}...' " 
        f"JOB DESCRIPTION: '{job_description}' "
        f"Based on this analysis, provide the requested structured JSON output."
    )

    # JSON Schema for structured output (ensures reliable parsing)
    response_schema = {
        "type": "OBJECT",
        "properties": {
            "JobReadinessScore": {"type": "INTEGER", "description": "An overall ranking score from 1 to 100 on how job-ready the candidate is based on skills."},
            "Summary": {"type": "STRING", "description": "A concise summary of the candidate's strengths and the main skill gap identified."},
            "LearningPathSteps": {
                "type": "ARRAY",
                "description": "A 5-step actionable learning path to close the skill gaps.",
                "items": {
                    "type": "OBJECT",
                    "properties": {
                        "StepNumber": {"type": "INTEGER"},
                        "SkillArea": {"type": "STRING", "description": "The specific skill area to focus on (e.g., Advanced SQL, Cloud Security)."},
                        "Action": {"type": "STRING", "description": "The recommended action (e.g., Complete an online certification, Build a portfolio project, Read a foundational book)."}
                    },
                    "propertyOrdering": ["StepNumber", "SkillArea", "Action"]
                }
            }
        },
        "propertyOrdering": ["JobReadinessScore", "Summary", "LearningPathSteps"]
    }

    payload = {
        "contents": [{ "parts": [{ "text": user_query }] }],
        "systemInstruction": { "parts": [{ "text": system_prompt }] },
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": response_schema
        }
    }

    max_retries = 3
    delay = 1
    for attempt in range(max_retries):
        try:
            response = requests.post(
                f"{GEMINI_API_URL}?key={GEMINI_API_KEY}",
                headers={"Content-Type": "application/json"},
                data=json.dumps(payload),
                timeout=45 
            )
            response.raise_for_status() 

            # Process the response
            result = response.json()
            # Extract the raw JSON string from the response
            json_text = result['candidates'][0]['content']['parts'][0]['text']
            parsed_json = json.loads(json_text)
            return parsed_json

        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                time.sleep(delay)
                delay *= 2
            else:
                st.error(f"Failed to get AI analysis after {max_retries} attempts. API/Network Error: {e}")
                return None
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            # Catch errors in parsing the expected JSON format
            st.error(f"AI response received but failed to parse the structured JSON. Error: {e}")
            return None
    return None

# Main Application Logic 
def main():
    uploaded_file = st.file_uploader("Upload your Resume (ðŸ“‘PDF)", type=["pdf"])
    job_description = st.text_area("Paste the Job Description here", height=200)
    
    if st.button("Analyze Resume", type="primary"):

        if not GEMINI_API_KEY:
            st.error("Cannot run analysis. Please check your `secrets.toml` file.")
            return

        if not uploaded_file:
            st.warning("Please upload your resume PDF file.")
            return
        if not job_description.strip():
            st.warning("Please enter the job description.")
            return

        with st.spinner("Analyzing resume and generating AI recommendations..."):
            resume_text = extract_text_from_pdf(uploaded_file)
            if not resume_text:
                st.error("Failed to extract text from the uploaded PDF. Please try again with a clear, text-based PDF.")
                return
            
            
            similarity_score, _, _ = calculate_similaity(resume_text, job_description)
            
            
            candidate_info = extract_candidate_info(resume_text)
            
            
            ALL_HEADERS = ['SKILLS', 'PROJECTS', 'EXPERIENCE', 'WORK HISTORY', 'EDUCATION', 'SUMMARY', 'CONTACT', 'AWARDS', 'CERTIFICATIONS', 'PUBLICATIONS']
            skill_text = extract_section_content(resume_text, 'SKILLS', [h for h in ALL_HEADERS if h != 'SKILLS'])
            project_text = extract_section_content(resume_text, 'PROJECTS', [h for h in ALL_HEADERS if h != 'PROJECTS'])
            
            
            if not project_text:
                project_text = extract_section_content(resume_text, 'WORK HISTORY', [h for h in ALL_HEADERS if h != 'WORK HISTORY'])

            
            ranked_skills = rank_skills_by_projects(skill_text, project_text)

            # Results Display
            st.subheader("Analysis Results")
            
            # Display Candidate Details
            st.markdown("### ðŸ‘¤ Candidate Details")
            col_name, col_email, col_phone = st.columns(3)
            col_name.metric("Name", candidate_info["Name"])
            col_email.metric("Email", candidate_info["Email"])
            col_phone.metric("Phone", candidate_info["Phone"])
            st.markdown("---")
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.metric("TF-IDF Compatibility Score", f"{similarity_score}%")
                
                # --- DENSITY CURVE CHART (Replaces PIE CHART) ---
                st.markdown("##### Overall Skill Relevance Distribution")
                
                # Generate and display the new Density Curve plot
                density_fig = plot_density_curve(similarity_score)
                st.pyplot(density_fig)
                
            with col2:
                st.markdown("### Match Alignment")
                # Define match_color based on score for consistent display
                if similarity_score < 40:
                    match_color = '#ff4b4b' # Red
                    st.warning("Low Match: Your resume needs significant tailoring. Focus on matching job keywords.")
                elif similarity_score < 70:
                    match_color = '#ffa726' # Orange
                    st.info("Good Match: Your resume aligns fairly well. A few key updates could greatly improve your chances.")
                else:
                    match_color = '#0f9d58' # Green (Google Green)
                    st.success("Excellent Match! Your resume strongly aligns with the job description. Proceed to the AI analysis for final tuning.")
                
                st.markdown("---")
                st.markdown("### ðŸ› ï¸ Extracted Skills & Projects")
                
                st.markdown("##### Skills Ranked by Project Usage")
                if ranked_skills:
                    rank_data = [{"Skill": skill.capitalize(), "Frequency in Projects": count} for skill, count in ranked_skills if count > 0]
                    if rank_data:
                        st.dataframe(rank_data, use_container_width=True, hide_index=True)
                    else:
                        st.info("No skills found in the 'Skills' section were explicitly mentioned in the 'Projects' section.")
                else:
                    st.info("Could not reliably rank skills by project usage. Ensure your resume has clear 'Skills' and 'Projects' sections.")
                    
                st.markdown("##### Raw Extracted Section Content")
                
                with st.expander("View Raw Extracted Skills"):
                    st.text(skill_text if skill_text else "No 'Skills' section found.")
                with st.expander("View Raw Extracted Projects"):
                    st.text(project_text if project_text else "No 'Projects' or 'Work History' section found.")

            st.markdown("---")
            
            # 1. AI-Powered Learning Path Generation
            st.header("ðŸ§  AI-Powered Analysis and Recommendations")
            
            ai_results = generate_learning_path(resume_text, job_description)

            if ai_results:
                
                readiness_score = ai_results.get("JobReadinessScore", "N/A") 
                summary = ai_results.get("Summary", "AI analysis summary is unavailable.")
                path_steps = ai_results.get("LearningPathSteps", [])

                # Redefine match_color here based on similarity_score in case it was used in col1/col2
                if similarity_score < 40:
                    match_color = '#ff4b4b' # Red
                elif similarity_score < 70:
                    match_color = '#ffa726' # Orange
                else:
                    match_color = '#0f9d58' # Green
                    
                st.markdown(f"**Job Readiness Score (AI Ranking):** <span style='font-size: 2.5em; color: {match_color};'>**{readiness_score}/100**</span>", unsafe_allow_html=True)
                
                # Display Summary
                st.info(f"**AI Summary:** {summary}")
                
                # Display Learning Path
                st.markdown("---")
                st.markdown("### ðŸš€ Recommended 5-Step Learning Path to Close Gaps")
                
                if path_steps:
                    st.markdown(
                        """
                        <style>
                        .path-step {
                            padding: 15px;
                            border-radius: 10px;
                            margin-bottom: 10px;
                            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                            background-color: #f0f8ff; /* Light blue/white background */
                        }
                        .step-skill {
                            font-size: 1.1em;
                            font-weight: bold;
                            color: #1f77b4; /* A dark blue for skill focus */
                        }
                        .step-action {
                            font-style: italic;
                            margin-top: 5px;
                        }
                        </style>
                        """,
                        unsafe_allow_html=True
                    )
                    
                    for step in path_steps:
                        st.markdown(
                            f"""
                            <div class="path-step">
                                ðŸ“š **Step {step.get('StepNumber', 'N/A')}:** <span class="step-skill">{step.get('SkillArea', 'Unknown Area')}</span>
                                <div class="step-action">Action: {step.get('Action', 'No action provided.')}</div>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                else:
                    st.warning("The AI could not generate a detailed 5-step learning path. Please refine your input or check the job description length.")
            else:
                st.error("AI analysis could not be completed. Please check the API key, or try again later.")

if __name__=="__main__":
    main()
